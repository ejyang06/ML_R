---
output: html_document
---
Name:Elva Yang
Date:
Purpose/Project: 
knitr 
@ R version 3.4.2


```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

#load packages
```{r}
rm(list = ls())
#devtools::session_info()

#install/load packages
packages_lib<- c("dplyr", "data.table", "ggplot2","reshape2","readr","caret","sqldf","readxl", "tidyquant","wordcloud","rmarkdown", "knitr",                 "devtools","readxl")
packages_ML<- c("e1071", "neuralnet", "cvAUC", "lime","h2o","h2oEnsemble")

#install.packages(packages_lib)
#install.packages(packages_ML)

# Load packages
lapply(packages_lib, require, character.only = TRUE)
lapply(packages_ML, require, character.only = TRUE)

# Github pakcages
library(devtools)
install_github("h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package")
library(knitr)
```

#Import Data

```{r load_data}
WA_Fn_UseC_HR_Employee_Attrition <- read.csv("~/Desktop/WA_Fn-UseC_-HR-Employee-Attrition.csv")
Data<-WA_Fn_UseC_HR_Employee_Attrition
View(Data)
attach(Data)
```


# Data Exploratory
``` {r data explor}
summary(Data)
str(Data)
Data[1:10,] %>%
    knitr::kable(caption = "First 10 rows")

Data <- Data %>%
    mutate_if(is.character, as.factor) %>%
    select(Attrition, everything())

```



#Data Visualization
``` {r data clean}



```

# Data Cleaning
```{r data viz}


```


# Analysis Setup
``` {r modeling}
# Init h2o
h2o.init()
h2o.removeAll() 

#split data into Train/Val/Test
Data_h2o<-as.h2o(Data)
split_h2o<-h2o.splitFrame(Data_h2o,c(0.7,0.15),seed=1234)
split_h2o

train_h2o<-h2o.assign(split_h2o[[1]],"train") # 70%
val_h2o<-h2o.assign(split_h2o[[2]],"Val")  # 15%
test_h2o<-h2o.assign(split_h2o[[3]],"test") # 15%

y <- "Attrition" # target name
x<-setdiff(names(train_h2o),y) #feature names

# Random Forest - for getting important variables
Data_h2o_rf = h2o.randomForest(y = y, x = x, training_frame = train_h2o)
impvariables = h2o.varimp(Data_h2o_rf)
View(impvariables)


## The response is encoded as factor for binary classification
topScaledImp<-filter(impvariables, scaled_importance >=0.6) #selecting features having scaled_imp > 0.6
topImpVar<-topScaledImp$variable

```

# Analysis Set up - learners library
```{r}
h2o.glm.1 <- function(..., alpha = 0.0) h2o.glm.wrapper(..., alpha = alpha)
h2o.glm.2 <- function(..., alpha = 0.5) h2o.glm.wrapper(..., alpha = alpha)
h2o.glm.3 <- function(..., alpha = 1.0) h2o.glm.wrapper(..., alpha = alpha)

h2o.randomForest.1 <- function(..., ntrees = 200, nbins = 50, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, nbins = nbins, seed = seed)
h2o.randomForest.2 <- function(..., ntrees = 200, sample_rate = 0.75, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, sample_rate = sample_rate, seed = seed)
h2o.randomForest.3 <- function(..., ntrees = 200, sample_rate = 0.85, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, sample_rate = sample_rate, seed = seed)
h2o.randomForest.4 <- function(..., ntrees = 200, nbins = 50, balance_classes = TRUE, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, nbins = nbins, balance_classes = balance_classes, seed = seed)

h2o.gbm.1 <- function(..., ntrees = 100, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, seed = seed)
h2o.gbm.2 <- function(..., ntrees = 100, nbins = 50, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, nbins = nbins, seed = seed)
h2o.gbm.3 <- function(..., ntrees = 100, max_depth = 10, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, max_depth = max_depth, seed = seed)
h2o.gbm.4 <- function(..., ntrees = 100, col_sample_rate = 0.8, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed)
h2o.gbm.5 <- function(..., ntrees = 100, col_sample_rate = 0.7, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed)
h2o.gbm.6 <- function(..., ntrees = 100, col_sample_rate = 0.6, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed)
h2o.gbm.7 <- function(..., ntrees = 100, balance_classes = TRUE, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, balance_classes = balance_classes, seed = seed)
h2o.gbm.8 <- function(..., ntrees = 100, max_depth = 3, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, max_depth = max_depth, seed = seed)

h2o.deeplearning.1 <- function(..., hidden = c(500,500), activation = "Rectifier", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.2 <- function(..., hidden = c(200,200,200), activation = "Tanh", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.3 <- function(..., hidden = c(500,500), activation = "RectifierWithDropout", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.4 <- function(..., hidden = c(500,500), activation = "Rectifier", epochs = 50, balance_classes = TRUE, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, balance_classes = balance_classes, seed = seed)
h2o.deeplearning.5 <- function(..., hidden = c(100,100,100), activation = "Rectifier", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.6 <- function(..., hidden = c(50,50), activation = "Rectifier", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.7 <- function(..., hidden = c(100,100), activation = "Rectifier", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
```

# Model fitting
```{r}
# H2o Default learners
metalearner <- "h2o.glm.wrapper"

default_learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper", 
             "h2o.gbm.wrapper", "h2o.deeplearning.wrapper")


default_learner_fit <- h2o.ensemble(x = topImpVar, y = y, 
                    training_frame = train_h2o, 
                    family = 'binomial', 
                    learner = default_learner, 
                    metalearner = metalearner,
                    cvControl = list(V = 5))

# Check default learners Performance
perf <- h2o.ensemble_performance(default_learner_fit, newdata = val_h2o)
print(perf, metric = "AUC")



# New learner fit (customize parameters)
new_learner <- c("h2o.glm.wrapper",
             "h2o.randomForest.1", "h2o.randomForest.2",
             "h2o.gbm.1", "h2o.gbm.6", "h2o.gbm.8",
             "h2o.deeplearning.1", "h2o.deeplearning.6", "h2o.deeplearning.7")


new_learner_fit <- h2o.ensemble(x = topImpVar, y = y, 
                    training_frame = train_h2o, 
                    family = 'binomial', 
                    learner = new_learner, 
                    metalearner = metalearner,
                    cvControl = list(V = 5))

# Check new learners Performance
perf <- h2o.ensemble_performance(new_learner_fit, newdata = val_h2o)
print(perf, metric = "AUC")





# Finalizing Models
fit<-new_learner_fit
learner<-new_learner

#Check on Validation datasets
valpred <- predict(fit, val_h2o)

#third column is P(Y==1)
valpredictions <- as.data.frame(valpred$pred)[,3]
labels <- as.data.frame(val_h2o[,y])[,1]

#AUC expected
cvAUC::AUC(predictions = valpredictions, labels = labels)

# Check how each learner did, tuning the parameters in the future
L <- length(learner)
L
auc <- sapply(seq(L), function(l) cvAUC::AUC(predictions = as.data.frame(valpred$basepred)[,l], labels = labels)) 
learner_auc<-data.frame(learner, auc)


# Generate predictions on the test set:
pred <- predict(fit, test_h2o)
predictions <- as.data.frame(pred$pred)

```

# Generate output
```{r Generate_output}
# predictions<-sqldf('select Yes, prediction from predictions' )

output_data <-function (data, predictions, filename) {
  
  predictions<-predictions %>% select(-No)
  pred_result<-as.data.frame(predictions)
  test_r<-as.data.frame(test_h2o)
  output<-data.frame(test_r, pred_result)
  write.csv(output,filename,row.names = FALSE )
}

output_data(test_h2o,predictions,"output_model.csv")

```

# Automate Parameter turning
```{r}
# automated parameter tuning of C5.0 decision tree 
set.seed(1234)
(l <- sapply(Data, function(x) is.factor(x))) # check variable whether is a factor
f <- Data[, l]
drop<-print.data.frame(ifelse(n <- sapply(f, function(x) length(levels(x))) == 1, "DROP", "NODROP")) # check which factor is ONE level
drop
m <- train(Attrition ~., data=Data[ ,!(names(Data) %in% c('Over18'))], method = "C5.0") # apply decision tree and removing factor is ONE level
m

```

```{r}
#library("knitr")
#knit2html("file")
#rmarkdown::render('file.rmd', output_format = 'html_document')
```

# Discussion




